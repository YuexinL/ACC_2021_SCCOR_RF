---
title: "Untitled"
output: html_document
---

```{r}
# getwd()
# save.image(file = "SVM.RData")
```

```{r}
# rm(list = ls())
# load("SVM.RData")
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

```{r}
library(pacman)
p_load(readxl, tidyverse, RSBID, 
       caret, pROC, plotROC)
select <- dplyr::select
```

```{r, warning = T}
setwd(dirname(dirname(rstudioapi::getActiveDocumentContext()$path)))
# getwd()
load("Processed Data/data_sym.RData")
load("Processed Data/data_asym.RData")
```

# Data Preparation

```{r}
# relabel response classes
d1 <- data_sym %>% 
  select(-Patient_ID) %>% 
  mutate(LVEF_m06 = factor(LVEF_m06, levels = c(">50", "<50"), labels = c("normal", "low")))

# record the original order of features
predictors <- colnames(d1)
```

```{r}
# transform categorical variables into dummy variables (for LASSO)
d2 <- d1 %>% 
  mutate(Gender = ifelse(Gender == "F", 0, 1), 
         Hypertension = ifelse(Hypertension == "no", 0, 1), 
         AF = ifelse(AF == "no", 0, 1))
```

## Support Vector Machines

### Baseline

#### SVM with Linear Kernel

```{r}
start.time <- Sys.time()

# data input
input <- d1

# repeated LGOCV cross validation
pct <- 0.85
rep <- 100

control <- trainControl(
  method = "LGOCV",
  p = pct, 
  number = rep,
  savePredictions = "all", 
  classProbs = TRUE, 
  summaryFunction = twoClassSummary)

grid <- expand.grid(
  .C = 10^(-2:2))

# Fit the model
set.seed(123)
fit <- train(
  LVEF_m06 ~ ., 
  data = input, 
  method = "svmLinear", 
  metric = "ROC",
  tuneGrid = grid, 
  trControl = control)

end.time <- Sys.time()
print(end.time - start.time)

assign(paste0("svm1", "_baseline"), 
       fit)
```

```{r}
fit <- get(paste0("svm1", "_baseline"))

fit$resampledCM %>%
  group_by(C) %>% 
  summarise(Accuracy = mean((cell1+cell4) / (cell1+cell2+cell3+cell4)),
            Precision = mean(cell1 / (cell1+cell3)),
            Recall = mean(cell1 / (cell1+cell2)),
            Specificity = mean(cell4 / (cell3+cell4)), 
            .groups = "drop") %>% 
  ungroup() %>% 
  as.data.frame() %>% 
  mutate(F1 = 2 / (1/Recall+1/Precision)) %>% 
  left_join(fit$results[, 1:2], by = c("C")) %>% 
  arrange(desc(ROC)) -> tune

tune

assign(paste0("tune", ".svm1_baseline"), 
       tune)
```

#### SVM with Polynomial Kernel

```{r}
start.time <- Sys.time()

# data input
input <- d1

# repeated LGOCV cross validation
pct <- 0.85
rep <- 100

control <- trainControl(
  method = "LGOCV",
  p = pct, 
  number = rep,
  savePredictions = "all", 
  classProbs = TRUE, 
  summaryFunction = twoClassSummary)

grid <- expand.grid(
  .degree = 1:3, 
  .scale = 10^(-3:-1), 
  .C = 10^(-2:2))

# Fit the model
set.seed(123)
fit <- train(
  LVEF_m06 ~ ., 
  data = input, 
  method = "svmPoly", 
  metric = "ROC",
  tuneGrid = grid, 
  trControl = control)

end.time <- Sys.time()
print(end.time - start.time)

assign(paste0("svm2", "_baseline"), 
       fit)
```

```{r}
fit <- get(paste0("svm2", "_baseline"))

fit$resampledCM %>%
  group_by(degree, scale, C) %>% 
  summarise(Accuracy = mean((cell1+cell4) / (cell1+cell2+cell3+cell4)),
            Precision = mean(cell1 / (cell1+cell3)),
            Recall = mean(cell1 / (cell1+cell2)),
            Specificity = mean(cell4 / (cell3+cell4)), 
            .groups = "drop") %>% 
  ungroup() %>% 
  as.data.frame() %>% 
  mutate(F1 = 2 / (1/Recall+1/Precision)) %>% 
  left_join(fit$results[, 1:4], by = c("degree", "scale", "C")) %>% 
  arrange(desc(ROC)) -> tune

tune

assign(paste0("tune", ".svm2_baseline"), 
       tune)
```

#### SVM with Radial Basis Function Kernel

```{r}
start.time <- Sys.time()

# data input
input <- d1

# repeated LGOCV cross validation
pct <- 0.85
rep <- 100

control <- trainControl(
  method = "LGOCV",
  p = pct, 
  number = rep,
  savePredictions = "all", 
  classProbs = TRUE, 
  summaryFunction = twoClassSummary)

grid <- expand.grid(
  .sigma = 10^(-2:3), 
  .C = 10^(-2:2))

# Fit the model
set.seed(123)
fit <- train(
  LVEF_m06 ~ ., 
  data = input, 
  method = "svmRadial", 
  metric = "ROC",
  tuneGrid = grid, 
  trControl = control)

end.time <- Sys.time()
print(end.time - start.time)

assign(paste0("svm3", "_baseline"), 
       fit)
```

```{r}
fit <- get(paste0("svm3", "_baseline"))

fit$resampledCM %>%
  group_by(sigma, C) %>% 
  summarise(Accuracy = mean((cell1+cell4) / (cell1+cell2+cell3+cell4)),
            Precision = mean(cell1 / (cell1+cell3)),
            Recall = mean(cell1 / (cell1+cell2)),
            Specificity = mean(cell4 / (cell3+cell4)), 
            .groups = "drop") %>% 
  ungroup() %>% 
  as.data.frame() %>% 
  mutate(F1 = 2 / (1/Recall+1/Precision)) %>% 
  left_join(fit$results[, 1:3], by = c("sigma", "C")) %>% 
  arrange(desc(ROC)) -> tune

tune

assign(paste0("tune", ".svm3_baseline"), 
       tune)
```

### Feature Selection: RFE (Recursive Feature Elimination)

```{r}
imp.svm_baseline <- varImp(svm2_baseline)$importance %>% arrange(desc(normal))
imp.svm_baseline
```

```{r}
start.time <- Sys.time()

# data input
input <- d1
imp <- imp.svm_baseline
rank <- rownames(imp)[1:10]

# cv recursion
rfe <- NULL

for (m in 2:length(rank)){
  
  # repeated LGOCV cross validation
  pct <- 0.85
  rep <- 100
  
  control <- trainControl(
    method = "LGOCV",
    p = pct, 
    number = rep,
    savePredictions = "all", 
    classProbs = TRUE, 
    summaryFunction = twoClassSummary)
  
  grid <- expand.grid(
    .degree = 1:3, 
    .scale = 10^(-3:-1), 
    .C = 10^(-2:2))
  
  # model training
  set.seed(123)
  fit <- train(
    LVEF_m06 ~ .,
    data = input %>% select(LVEF_m06, which(predictors %in% rank[1:m])),
    method = "svmPoly",
    metric = "ROC",
    tuneGrid = grid,
    trControl = control)
  
  fit$resampledCM %>%
    group_by(degree, scale, C) %>% 
    summarise(Accuracy = mean((cell1+cell4) / (cell1+cell2+cell3+cell4)),
              Precision = mean(cell1 / (cell1+cell3)),
              Recall = mean(cell1 / (cell1+cell2)),
              Specificity = mean(cell4 / (cell3+cell4)), 
              .groups = "drop") %>% 
    ungroup() %>% 
    as.data.frame() %>% 
    mutate(F1 = 2 / (1/Recall+1/Precision)) %>% 
    left_join(fit$results[, 1:4], by = c("degree", "scale", "C")) %>% 
    .[1, ]
    rbind(rfe, .) -> rfe
  
}

end.time <- Sys.time()
print(end.time - start.time)

rfe <- rfe %>%
  cbind(Variables = 2:length(rank), .) %>%
  arrange(desc(ROC))
rownames(rfe) <- NULL

assign(paste0("rfe", ".svm"),
       rfe)
```

```{r}
rfe.svm
```

### Final Model

```{r}
# optimal selection after elimination
start.time <- Sys.time()

# data input
input <- d1
imp <- imp.svm_baseline
rfe <- rfe.svm

rank <- rownames(imp)
m <- rfe[which.max(rfe$ROC), "Variables"]

# repeated LGOCV cross validation
pct <- 0.85
rep <- 100

control <- trainControl(
  method = "LGOCV",
  p = pct, 
  number = rep,
  savePredictions = "all", 
  classProbs = TRUE, 
  summaryFunction = twoClassSummary)

grid <- expand.grid(
  .degree = 1:3, 
  .scale = 10^(-3:-1), 
  .C = 10^(-2:2))

# model training
set.seed(123)
fit <- train(
  LVEF_m06 ~ .,
  data = input %>% select(LVEF_m06, which(predictors %in% rank[1:m])),
  method = "svmPoly",
  metric = "ROC",
  tuneGrid = grid, 
  trControl = control)

end.time <- Sys.time()
print(end.time - start.time)

assign(paste0("svm", "_final"), 
       fit)
```

```{r}
fit <- get(paste0("svm", "_final"))

fit$resampledCM %>%
  group_by(degree, scale, C) %>% 
  summarise(Accuracy = mean((cell1+cell4) / (cell1+cell2+cell3+cell4)),
            Precision = mean(cell1 / (cell1+cell3)),
            Recall = mean(cell1 / (cell1+cell2)),
            Specificity = mean(cell4 / (cell3+cell4)), 
            .groups = "drop") %>% 
  ungroup() %>% 
  as.data.frame() %>% 
  mutate(F1 = 2 / (1/Recall+1/Precision)) %>% 
  left_join(fit$results[, 1:4], by = c("degree", "scale", "C")) %>% 
  arrange(desc(ROC), desc(Accuracy)) -> tune

tune

assign(paste0("tune", ".svm_final"), 
       tune)
```

```{r}
imp.svm_final <- varImp(svm_final)$importance %>% arrange(desc(normal))
imp.svm_final
```




# Top 8

```{r}
# optimal selection after elimination
start.time <- Sys.time()

# data input
input <- d1
imp <- imp.svm_baseline
rfe <- rfe.svm

rank <- rownames(imp)
m <- 8

# repeated LGOCV cross validation
pct <- 0.85
rep <- 100

control <- trainControl(
  method = "LGOCV",
  p = pct, 
  number = rep,
  savePredictions = "all", 
  classProbs = TRUE, 
  summaryFunction = twoClassSummary)

grid <- expand.grid(
  .sigma = 10^(-2:3), 
  .C = 10^(-2:2))

# model training
set.seed(123)
fit <- train(
  LVEF_m06 ~ .,
  data = input %>% select(LVEF_m06, which(predictors %in% rank[1:m])),
  method = "svmRadial",
  metric = "ROC",
  tuneGrid = grid, 
  trControl = control)

end.time <- Sys.time()
print(end.time - start.time)

assign(paste0("svm", "8"), 
       fit)
```

```{r}
fit <- get(paste0("svm", "8"))

fit$resampledCM %>%
  group_by(sigma, C) %>% 
  summarise(Accuracy = mean((cell1+cell4) / (cell1+cell2+cell3+cell4)),
            Precision = mean(cell1 / (cell1+cell3)),
            Recall = mean(cell1 / (cell1+cell2)),
            Specificity = mean(cell4 / (cell3+cell4)), 
            F1 = 2 / (1/Recall+1/Precision)) %>% 
  arrange(desc(Accuracy)) %>% 
  as.data.frame() -> tune

tune

assign(paste0("tune", ".svm8"), 
       tune)
```

```{r}
imp.svm8 <- varImp(svm8)$importance %>% arrange(desc(normal))
imp.svm8
```



```{r}
setwd(dirname(getwd()))
getwd()
save(imp.svm_baseline, file = "Feature Importance/imp.svm_baseline.RData")
save(svm_final, file = "ROC/svm_final.RData")
```

```{r}
table(predict(svm_final, newdata = data_asym))
```








